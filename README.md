# Summary about the project

<!-- TABLE OF CONTENTS -->
<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href="#overview">overview</a></li>
    <li><a href="#clustering-methods">Clustering Methods</a></li>
    <li><a href="#ensemble-methods">Ensemble Methods</a></li>
    <li><a href="#logistic-Regression">Logistic Regression</a></li>
    <li><a href="#naive-bayes">Naive Bayes</a></li>
    <li><a href="#neural-networks">Neural Networks</a></li>
  </ol>
</details>


<!-- Overview -->
# Overview 

This repository is a collection of different machine learning algorithms used while equipping knowledge on Machine Learning and coding, [repository.](https://github.com/Ganesamanian/Machine-Learning-algorithms#neural-networks).

<p align="right">(<a href="#top">back to top</a>)</p>

<!-- Clustering Methods -->
# Clustering Methods

The clustering methods used [here](https://github.com/Ganesamanian/Machine-Learning-algorithms/blob/main/Clustering%20methods/K-means%20clustering%20and%20EM.ipynb) are K-means and Mixture of Gaussian. The k-means is used to compress the image based on color clustering. Whereas, Mixture of Gaussian is used to cluster the datapoints provided based on Expected maximization. K-means is developed from scratch and compared with the existing method in sklearn library.

Before compressing
![nao1](Clustering%20methods/NAORelease.jpg)
After compressing
![nao](Clustering%20methods/Kmeans_NAORelease.jpg)
<p align="right">(<a href="#top">back to top</a>)</p>

<!-- Ensemble Methods -->
# Ensemble Methods

Ensemble Methods used [here](https://github.com/Ganesamanian/Machine-Learning-algorithms/blob/main/Ensemble%20Methods/Random%20forest%20and%20Adaboost.ipynb) are Random forest and Adaboost on housing data. This is more or like a comparison between the performance of both the methods analysed using complexity tree.


<p align="right">(<a href="#top">back to top</a>)</p>

<!-- Logistic Regression -->
# Logistic Regression

Logistic regression is a linear classifier which is used on three different data to classify them. Three datasets requires different preprocessing methods. Logistic regression is developed from scratch and compared with the existing method in sklearn library. You can find the file [here](https://github.com/Ganesamanian/Machine-Learning-algorithms/blob/main/Logistic%20Regression/Linear%20discriminant%20and%20Logistic%20Regression.ipynb)


<p align="right">(<a href="#top">back to top</a>)</p>

<!-- Naive Bayes -->
# Naive Bayes

Naive Bayes is the basic or initial classifier one would learn while starting with machine learning. Here Naive Bayes is used for spam classification of emails using sklearn library. Also observed and experimented how to cheat the classifier. You can find the file [here](https://github.com/Ganesamanian/Machine-Learning-algorithms/blob/main/Naive%20Bayes/Spam%20classification%20using%20Naive%20Bayes.ipynb)
<p align="right">(<a href="#top">back to top</a>)</p>

<!-- Neural Networks -->
# Neural Networks

Developed a basic three layer artificial neural network for hand written number classification from minx dataset. Also there includes experiment on classification change based on the hyperparameter tuning. You can fine the file [here](https://github.com/Ganesamanian/Machine-Learning-algorithms/tree/main/Neural%20Networks)
<p align="right">(<a href="#top">back to top</a>)</p>


